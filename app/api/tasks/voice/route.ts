// Force dynamic rendering to avoid build-time issues
export const dynamic = "force-dynamic";
export const runtime = "nodejs";

/**
 * Voice Task Creation API Route
 *
 * Handles voice recording uploads, transcription, and task creation from voice input.
 */

import { SpanStatusCode, trace } from "@opentelemetry/api";
import { type NextRequest, NextResponse } from "next/server";
import { ulid } from "ulid";
import { z } from "zod";
import { db } from "@/db/config";
import { tasks } from "@/db/schema";
import { observability } from "@/lib/observability";
import {
	createApiErrorResponse,
	createApiSuccessResponse,
} from "@/src/schemas/api-routes";
import { VoiceTaskCreationSchema } from "@/src/schemas/enhanced-task-schemas";

// Mock transcription service (would integrate with OpenAI Whisper, Google Speech-to-Text, etc.)
const transcribeAudio = async (
	audioFile: File,
): Promise<{
	text: string;
	confidence: number;
	language: string;
	duration: number;
}> => {
	// Simulate transcription processing
	await new Promise((resolve) => setTimeout(resolve, 1500));

	// Mock transcription result
	return {
		text: "Create a new task to fix the login bug on the dashboard page. It should be high priority and assigned to the frontend team.",
		confidence: 0.95,
		language: "en-US",
		duration: audioFile.size / 1000, // Mock duration calculation
	};
};

// Parse transcribed text to extract task details using NLP (mock implementation)
const parseTaskFromTranscription = (text: string) => {
	// In real implementation, would use NLP to extract:
	// - Task title
	// - Description
	// - Priority keywords
	// - Assignee mentions
	// - Due date mentions
	// - Labels/tags

	const priority = /high.?priority|urgent|critical/i.test(text)
		? "high"
		: /low.?priority|minor/i.test(text)
			? "low"
			: "medium";

	const assigneeMatch = text.match(/assign(?:ed)?\s+to\s+(\w+(?:\s+\w+)?)/i);
	const assignee = assigneeMatch ? assigneeMatch[1] : undefined;

	const dueDateMatch = text.match(
		/(?:due|deadline|by)\s+([a-zA-Z]+\s+\d{1,2}(?:,?\s+\d{4})?)/i,
	);
	const dueDate = dueDateMatch ? dueDateMatch[1] : undefined;

	// Extract potential labels
	const labels = [];
	if (/bug|fix|error|issue/i.test(text)) {
		labels.push("bug");
	}
	if (/feature|enhancement|new/i.test(text)) {
		labels.push("feature");
	}
	if (/frontend|ui|interface/i.test(text)) {
		labels.push("frontend");
	}
	if (/backend|api|server/i.test(text)) {
		labels.push("backend");
	}
	if (/testing|test|qa/i.test(text)) {
		labels.push("testing");
	}

	// Generate title from first sentence or key phrases
	const sentences = text.split(/[.!?]+/).filter((s) => s.trim().length > 0);
	const title = sentences[0]?.trim() || "Voice-created task";

	return {
		title,
		description: text,
		priority,
		assignee,
		dueDate,
		labels: labels.length > 0 ? labels : ["voice-created"],
	};
};

/**
 * POST /api/tasks/voice - Create task from voice recording
 */
export async function POST(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.create");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;
		const requestData = JSON.parse(formData.get("data") as string);

		// Validate request data
		const validatedData = VoiceTaskCreationSchema.parse(requestData);

		if (!audioFile) {
			return NextResponse.json(
				createApiErrorResponse(
					"Audio file is required",
					400,
					"MISSING_AUDIO_FILE",
				),
				{ status: 400 },
			);
		}

		// Transcribe audio
		const transcription = await transcribeAudio(audioFile);

		// Parse task details from transcription
		const parsedTask = parseTaskFromTranscription(transcription.text);

		// Create task with voice metadata
		const newTask = {
			id: ulid(),
			title: parsedTask.title,
			description: parsedTask.description,
			status: "todo" as const,
			priority: parsedTask.priority as "low" | "medium" | "high",
			userId: validatedData.userId,
			assignee: parsedTask.assignee || validatedData.defaultAssignee,
			labels: parsedTask.labels,
			dueDate: parsedTask.dueDate ? new Date(parsedTask.dueDate) : undefined,
			metadata: {
				type: "voice_created",
				voiceRecording: {
					originalText: transcription.text,
					confidence: transcription.confidence,
					language: transcription.language,
					duration: transcription.duration,
					audioUrl: `https://storage.app.com/voice/${ulid()}.${audioFile.name.split(".").pop()}`,
					timestamp: new Date().toISOString(),
				},
				parsedData: parsedTask,
				processingFlags: {
					requiresReview: transcription.confidence < 0.8,
					lowConfidence: transcription.confidence < 0.7,
					autoGenerated: true,
				},
			},
			createdAt: new Date(),
			updatedAt: new Date(),
		};

		const [createdTask] = await db.insert(tasks).values(newTask).returning();

		// Record event
		await observability.events.collector.collectEvent(
			"user_action",
			"info",
			`Voice task created: ${createdTask.title}`,
			{
				taskId: createdTask.id,
				userId: createdTask.userId,
				transcriptionConfidence: transcription.confidence,
				audioDuration: transcription.duration,
				requiresReview: transcription.confidence < 0.8,
			},
			"api",
			["tasks", "voice", "creation"],
		);

		span.setAttributes({
			"task.id": createdTask.id,
			"task.type": "voice_created",
			"transcription.confidence": transcription.confidence,
			"audio.duration": transcription.duration,
			"processing.requires_review": transcription.confidence < 0.8,
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					task: createdTask,
					transcription: {
						text: transcription.text,
						confidence: transcription.confidence,
						language: transcription.language,
					},
					parsedData: parsedTask,
				},
				"Voice task created successfully",
			),
			{ status: 201 },
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		if (error instanceof z.ZodError) {
			return NextResponse.json(
				createApiErrorResponse(
					"Validation failed",
					400,
					"VALIDATION_ERROR",
					error.issues,
				),
				{ status: 400 },
			);
		}

		observability.metrics.errorRate(1, "voice_api");

		return NextResponse.json(
			createApiErrorResponse(
				"Failed to create voice task",
				500,
				"CREATE_VOICE_TASK_ERROR",
			),
			{ status: 500 },
		);
	} finally {
		span.end();
	}
}

/**
 * POST /api/tasks/voice/transcribe - Transcribe audio without creating task
 */
export async function PUT(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.transcribe");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;

		if (!audioFile) {
			return NextResponse.json(
				createApiErrorResponse(
					"Audio file is required",
					400,
					"MISSING_AUDIO_FILE",
				),
				{ status: 400 },
			);
		}

		// Transcribe audio
		const transcription = await transcribeAudio(audioFile);

		// Parse task suggestions from transcription
		const suggestions = parseTaskFromTranscription(transcription.text);

		span.setAttributes({
			"transcription.confidence": transcription.confidence,
			"audio.duration": transcription.duration,
			"transcription.language": transcription.language,
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					transcription,
					suggestions,
				},
				"Audio transcribed successfully",
			),
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		return NextResponse.json(
			createApiErrorResponse(
				"Failed to transcribe audio",
				500,
				"TRANSCRIPTION_ERROR",
			),
			{ status: 500 },
		);
	} finally {
		span.end();
	}
}
