// Force dynamic rendering to avoid build-time issues
export const dynamic = "force-dynamic";
export const runtime = "nodejs";

/**
 * Voice Task Creation API Route
 *
 * Handles voice recording uploads, transcription, and task creation from voice input.
 */

import { SpanStatusCode, trace } from "@opentelemetry/api";
import { type NextRequest, NextResponse } from "next/server";
import { ulid } from "ulid";
import { z } from "zod";
import { db } from "@/db/config";
import { tasks } from "@/db/schema";
import { handleRouteError } from "@/lib/api/error-handlers";
import { observability } from "@/lib/observability";
import { parseTaskFromTranscription } from "@/lib/voice/task-parser-utils";
import { createApiErrorResponse, createApiSuccessResponse } from "@/src/schemas/api-routes";
import { VoiceTaskCreationSchema } from "@/src/schemas/enhanced-task-schemas";
import { transcriptionService } from "@/lib/services/transcription";

// Note: parseTaskFromTranscription is now imported from @/lib/voice/task-parser-utils

/**
 * POST /api/tasks/voice - Create task from voice recording
 */
export async function POST(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.create");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;
		const requestData = JSON.parse(formData.get("data") as string);

		// Validate request data
		const validatedData = VoiceTaskCreationSchema.parse(requestData);

		if (!audioFile) {
			return NextResponse.json(createApiErrorResponse("Audio file is required", 400), {
				status: 400,
			});
		}

		// Convert File to Blob for transcription service
		const audioBlob = new Blob([await audioFile.arrayBuffer()], { type: audioFile.type });

		// Transcribe audio using Gemini Flash 2.5
		const transcriptionResult = await transcriptionService.transcribe({
			audioBlob,
			language: validatedData.language || "en",
			format: "verbose_json",
		});

		// Extract task details from transcription using AI
		const parsedTask = await transcriptionService.extractTaskFromTranscription(
			transcriptionResult.text
		);

		// Add confidence score (estimate based on whether we got segments)
		const confidence = transcriptionResult.segments ? 0.95 : 0.85;

		// Create task with voice metadata
		const newTask = {
			id: ulid(),
			title: parsedTask.title,
			description: parsedTask.description,
			status: "todo" as const,
			priority: parsedTask.priority as "low" | "medium" | "high",
			userId: validatedData.userId,
			assignee: parsedTask.assignee || validatedData.defaultAssignee,
			labels: parsedTask.labels,
			dueDate: parsedTask.dueDate ? new Date(parsedTask.dueDate) : undefined,
			metadata: {
				type: "voice_created",
				voiceRecording: {
					originalText: transcriptionResult.text,
					confidence: confidence,
					language: transcriptionResult.language || validatedData.language || "en",
					duration: transcriptionResult.duration || audioFile.size / 16000, // Estimate if not provided
					audioUrl: `https://storage.app.com/voice/${ulid()}.${audioFile.name.split(".").pop()}`,
					timestamp: new Date().toISOString(),
					segments: transcriptionResult.segments,
				},
				parsedData: parsedTask,
				processingFlags: {
					requiresReview: confidence < 0.8,
					lowConfidence: confidence < 0.7,
					autoGenerated: true,
					transcriptionProvider: "gemini-2.0-flash",
				},
			},
			createdAt: new Date(),
			updatedAt: new Date(),
		};

		const [createdTask] = await db.insert(tasks).values(newTask).returning();

		// Record event
		await observability.events.collector.collectEvent(
			"user_action",
			"info",
			`Voice task created: ${createdTask.title}`,
			{
				taskId: createdTask.id,
				userId: createdTask.userId,
				transcriptionConfidence: confidence,
				audioDuration: transcriptionResult.duration || audioFile.size / 16000,
				requiresReview: confidence < 0.8,
				transcriptionProvider: "gemini-2.0-flash",
			},
			"api",
			["tasks", "voice", "creation"]
		);

		span.setAttributes({
			"task.id": createdTask.id,
			"task.type": "voice_created",
			"transcription.confidence": confidence,
			"audio.duration": transcriptionResult.duration || audioFile.size / 16000,
			"processing.requires_review": confidence < 0.8,
			"transcription.provider": "gemini-2.0-flash",
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					task: createdTask,
					transcription: {
						text: transcriptionResult.text,
						confidence: confidence,
						language: transcriptionResult.language || validatedData.language || "en",
						duration: transcriptionResult.duration,
						segments: transcriptionResult.segments,
					},
					parsedData: parsedTask,
				},
				"Voice task created successfully"
			),
			{ status: 201 }
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		if (error instanceof z.ZodError) {
			const mappedIssues = error.issues.map((issue) => ({
				field: issue.path.join(".") || "unknown",
				message: issue.message,
			}));
			return NextResponse.json(createApiErrorResponse("Validation failed", 400, mappedIssues), {
				status: 400,
			});
		}

		observability.metrics.errorRate.record(1);

		return NextResponse.json(createApiErrorResponse("Failed to create voice task", 500), {
			status: 500,
		});
	} finally {
		span.end();
	}
}

/**
 * POST /api/tasks/voice/transcribe - Transcribe audio without creating task
 */
export async function PUT(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.transcribe");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;

		if (!audioFile) {
			return NextResponse.json(createApiErrorResponse("Audio file is required", 400), {
				status: 400,
			});
		}

		// Convert File to Blob for transcription service
		const audioBlob = new Blob([await audioFile.arrayBuffer()], { type: audioFile.type });

		// Transcribe audio using Gemini Flash 2.5
		const transcriptionResult = await transcriptionService.transcribe({
			audioBlob,
			format: "verbose_json",
		});

		// Extract task suggestions from transcription using AI
		const suggestions = await transcriptionService.extractTaskFromTranscription(
			transcriptionResult.text
		);

		// Add confidence score
		const confidence = transcriptionResult.segments ? 0.95 : 0.85;

		span.setAttributes({
			"transcription.confidence": confidence,
			"audio.duration": transcriptionResult.duration || audioFile.size / 16000,
			"transcription.language": transcriptionResult.language || "en",
			"transcription.provider": "gemini-2.0-flash",
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					transcription: {
						text: transcriptionResult.text,
						confidence: confidence,
						language: transcriptionResult.language || "en",
						duration: transcriptionResult.duration,
						segments: transcriptionResult.segments,
					},
					suggestions,
				},
				"Audio transcribed successfully"
			)
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		return NextResponse.json(createApiErrorResponse("Failed to transcribe audio", 500), {
			status: 500,
		});
	} finally {
		span.end();
	}
}
