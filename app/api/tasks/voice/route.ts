// Force dynamic rendering to avoid build-time issues
export const dynamic = "force-dynamic";
export const runtime = "nodejs";

/**
 * Voice Task Creation API Route
 *
 * Handles voice recording uploads, transcription, and task creation from voice input.
 */

import { SpanStatusCode, trace } from "@opentelemetry/api";
import { type NextRequest, NextResponse } from "next/server";
import { ulid } from "ulid";
import { z } from "zod";
import { db } from "@/db/config";
import { tasks } from "@/db/schema";
import { handleRouteError } from "@/lib/api/error-handlers";
import { observability } from "@/lib/observability";
import { parseTaskFromTranscription } from "@/lib/voice/task-parser-utils";
import {
	createApiErrorResponse,
	createApiSuccessResponse,
} from "@/src/schemas/api-routes";
import { VoiceTaskCreationSchema } from "@/src/schemas/enhanced-task-schemas";

// Mock transcription service (would integrate with OpenAI Whisper, Google Speech-to-Text, etc.)
const transcribeAudio = async (
	audioFile: File,
): Promise<{
	text: string;
	confidence: number;
	language: string;
	duration: number;
}> => {
	// Simulate transcription processing
	await new Promise((resolve) => setTimeout(resolve, 1500));

	// Mock transcription result
	return {
		text: "Create a new task to fix the login bug on the dashboard page. It should be high priority and assigned to the frontend team.",
		confidence: 0.95,
		language: "en-US",
		duration: audioFile.size / 1000, // Mock duration calculation
	};
};

// Note: parseTaskFromTranscription is now imported from @/lib/voice/task-parser-utils

/**
 * POST /api/tasks/voice - Create task from voice recording
 */
export async function POST(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.create");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;
		const requestData = JSON.parse(formData.get("data") as string);

		// Validate request data
		const validatedData = VoiceTaskCreationSchema.parse(requestData);

		if (!audioFile) {
			return NextResponse.json(
				createApiErrorResponse(
					"Audio file is required",
					400,
					"MISSING_AUDIO_FILE",
				),
				{ status: 400 },
			);
		}

		// Transcribe audio
		const transcription = await transcribeAudio(audioFile);

		// Parse task details from transcription
		const parsedTask = parseTaskFromTranscription(transcription.text);

		// Create task with voice metadata
		const newTask = {
			id: ulid(),
			title: parsedTask.title,
			description: parsedTask.description,
			status: "todo" as const,
			priority: parsedTask.priority as "low" | "medium" | "high",
			userId: validatedData.userId,
			assignee: parsedTask.assignee || validatedData.defaultAssignee,
			labels: parsedTask.labels,
			dueDate: parsedTask.dueDate ? new Date(parsedTask.dueDate) : undefined,
			metadata: {
				type: "voice_created",
				voiceRecording: {
					originalText: transcription.text,
					confidence: transcription.confidence,
					language: transcription.language,
					duration: transcription.duration,
					audioUrl: `https://storage.app.com/voice/${ulid()}.${audioFile.name.split(".").pop()}`,
					timestamp: new Date().toISOString(),
				},
				parsedData: parsedTask,
				processingFlags: {
					requiresReview: transcription.confidence < 0.8,
					lowConfidence: transcription.confidence < 0.7,
					autoGenerated: true,
				},
			},
			createdAt: new Date(),
			updatedAt: new Date(),
		};

		const [createdTask] = await db.insert(tasks).values(newTask).returning();

		// Record event
		await observability.events.collector.collectEvent(
			"user_action",
			"info",
			`Voice task created: ${createdTask.title}`,
			{
				taskId: createdTask.id,
				userId: createdTask.userId,
				transcriptionConfidence: transcription.confidence,
				audioDuration: transcription.duration,
				requiresReview: transcription.confidence < 0.8,
			},
			"api",
			["tasks", "voice", "creation"],
		);

		span.setAttributes({
			"task.id": createdTask.id,
			"task.type": "voice_created",
			"transcription.confidence": transcription.confidence,
			"audio.duration": transcription.duration,
			"processing.requires_review": transcription.confidence < 0.8,
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					task: createdTask,
					transcription: {
						text: transcription.text,
						confidence: transcription.confidence,
						language: transcription.language,
					},
					parsedData: parsedTask,
				},
				"Voice task created successfully",
			),
			{ status: 201 },
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		if (error instanceof z.ZodError) {
			const mappedIssues = error.issues.map(issue => ({
				field: issue.path.join('.') || 'unknown',
				message: issue.message
			}));
			return NextResponse.json(
				createApiErrorResponse("Validation failed", 400, mappedIssues),
				{
					status: 400,
				},
			);
		}

		observability.metrics.errorRate(1, "voice_api");

		return NextResponse.json(
			createApiErrorResponse(
				"Failed to create voice task",
				500,
				"CREATE_VOICE_TASK_ERROR",
			),
			{ status: 500 },
		);
	} finally {
		span.end();
	}
}

/**
 * POST /api/tasks/voice/transcribe - Transcribe audio without creating task
 */
export async function PUT(request: NextRequest) {
	const tracer = trace.getTracer("tasks-api");
	const span = tracer.startSpan("tasks.voice.transcribe");

	try {
		const formData = await request.formData();
		const audioFile = formData.get("audio") as File;

		if (!audioFile) {
			return NextResponse.json(
				createApiErrorResponse(
					"Audio file is required",
					400,
					"MISSING_AUDIO_FILE",
				),
				{ status: 400 },
			);
		}

		// Transcribe audio
		const transcription = await transcribeAudio(audioFile);

		// Parse task suggestions from transcription
		const suggestions = parseTaskFromTranscription(transcription.text);

		span.setAttributes({
			"transcription.confidence": transcription.confidence,
			"audio.duration": transcription.duration,
			"transcription.language": transcription.language,
		});

		return NextResponse.json(
			createApiSuccessResponse(
				{
					transcription,
					suggestions,
				},
				"Audio transcribed successfully",
			),
		);
	} catch (error) {
		span.recordException(error as Error);
		span.setStatus({ code: SpanStatusCode.ERROR });

		return NextResponse.json(
			createApiErrorResponse(
				"Failed to transcribe audio",
				500,
				"TRANSCRIPTION_ERROR",
			),
			{ status: 500 },
		);
	} finally {
		span.end();
	}
}
